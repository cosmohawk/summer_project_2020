{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required modules\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elizabeth\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (0,1,2,6,10,15,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#creating a pandas dataframe with the CSV file provided\n",
    "df = pd.read_csv(r'C:\\Users\\elizabeth\\Documents\\journo_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>timezone</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>497378170757464065</td>\n",
       "      <td>497366362101415936</td>\n",
       "      <td>1407419171000</td>\n",
       "      <td>2014-08-07 13:46:11</td>\n",
       "      <td>UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ajjolley it's not a DfE account, it's some ra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '13265', 'username': 'RichardA'},...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>497376928446558208</td>\n",
       "      <td>497366362101415936</td>\n",
       "      <td>1407418875000</td>\n",
       "      <td>2014-08-07 13:41:15</td>\n",
       "      <td>UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ajjolley I agree. The new rules said lowfat m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '13265', 'username': 'RichardA'},...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>497368062665183232</td>\n",
       "      <td>497366362101415936</td>\n",
       "      <td>1407416761000</td>\n",
       "      <td>2014-08-07 13:06:01</td>\n",
       "      <td>UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ajjolley either a mistake or an elision. Milk...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '13265', 'username': 'RichardA'},...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>497349121632907264</td>\n",
       "      <td>497340002716897280</td>\n",
       "      <td>1407412245000</td>\n",
       "      <td>2014-08-07 11:50:45</td>\n",
       "      <td>UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@questionsit @Samfr indeed. Henrietta Barnet h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '13265', 'username': 'RichardA'},...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>497347223576133632</td>\n",
       "      <td>497340002716897280</td>\n",
       "      <td>1407411793000</td>\n",
       "      <td>2014-08-07 11:43:13</td>\n",
       "      <td>UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@questionsit @Samfr free schools aren't select...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'user_id': '13265', 'username': 'RichardA'},...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                  id     conversation_id     created_at  \\\n",
       "0          0  497378170757464065  497366362101415936  1407419171000   \n",
       "1          1  497376928446558208  497366362101415936  1407418875000   \n",
       "2          2  497368062665183232  497366362101415936  1407416761000   \n",
       "3          3  497349121632907264  497340002716897280  1407412245000   \n",
       "4          4  497347223576133632  497340002716897280  1407411793000   \n",
       "\n",
       "                  date timezone place  \\\n",
       "0  2014-08-07 13:46:11      UTC   NaN   \n",
       "1  2014-08-07 13:41:15      UTC   NaN   \n",
       "2  2014-08-07 13:06:01      UTC   NaN   \n",
       "3  2014-08-07 11:50:45      UTC   NaN   \n",
       "4  2014-08-07 11:43:13      UTC   NaN   \n",
       "\n",
       "                                               tweet hashtags cashtags  ...  \\\n",
       "0  @ajjolley it's not a DfE account, it's some ra...       []       []  ...   \n",
       "1  @ajjolley I agree. The new rules said lowfat m...       []       []  ...   \n",
       "2  @ajjolley either a mistake or an elision. Milk...       []       []  ...   \n",
       "3  @questionsit @Samfr indeed. Henrietta Barnet h...       []       []  ...   \n",
       "4  @questionsit @Samfr free schools aren't select...       []       []  ...   \n",
       "\n",
       "  geo  source user_rt_id user_rt  retweet_id  \\\n",
       "0 NaN     NaN        NaN     NaN         NaN   \n",
       "1 NaN     NaN        NaN     NaN         NaN   \n",
       "2 NaN     NaN        NaN     NaN         NaN   \n",
       "3 NaN     NaN        NaN     NaN         NaN   \n",
       "4 NaN     NaN        NaN     NaN         NaN   \n",
       "\n",
       "                                            reply_to retweet_date translate  \\\n",
       "0  [{'user_id': '13265', 'username': 'RichardA'},...          NaN       NaN   \n",
       "1  [{'user_id': '13265', 'username': 'RichardA'},...          NaN       NaN   \n",
       "2  [{'user_id': '13265', 'username': 'RichardA'},...          NaN       NaN   \n",
       "3  [{'user_id': '13265', 'username': 'RichardA'},...          NaN       NaN   \n",
       "4  [{'user_id': '13265', 'username': 'RichardA'},...          NaN       NaN   \n",
       "\n",
       "   trans_src  trans_dest  \n",
       "0        NaN         NaN  \n",
       "1        NaN         NaN  \n",
       "2        NaN         NaN  \n",
       "3        NaN         NaN  \n",
       "4        NaN         NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134143\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing - first glanses at the data \n",
    "column_names = list(df.columns)\n",
    "unique_values = list()\n",
    "for col in df:\n",
    "    unique_values.append(df[col].unique())\n",
    "number_unique_values = list()\n",
    "for col in df:\n",
    "    number_unique_values.append(len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[]', \"['#wagathachristie']\", \"['#abolisheton']\", ...,\n",
       "       \"['#tedxeuston']\", \"['#wad2012']\", \"['#lbs_ghcc2012']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#more glanses at the data\n",
    "unique_hashtags = df.hashtags.unique()\n",
    "unique_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column names</th>\n",
       "      <th>unique values</th>\n",
       "      <th>number of unique values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>19787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id</td>\n",
       "      <td>[497378170757464065, 497376928446558208, 49736...</td>\n",
       "      <td>134143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conversation_id</td>\n",
       "      <td>[497366362101415936, 497340002716897280, 49733...</td>\n",
       "      <td>108557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>created_at</td>\n",
       "      <td>[1407419171000, 1407418875000, 1407416761000, ...</td>\n",
       "      <td>133904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date</td>\n",
       "      <td>[2014-08-07 13:46:11, 2014-08-07 13:41:15, 201...</td>\n",
       "      <td>133904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>timezone</td>\n",
       "      <td>[UTC, stephenexley]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>place</td>\n",
       "      <td>[nan, University of Oxford, Belfast City Hall,...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweet</td>\n",
       "      <td>[@ajjolley it's not a DfE account, it's some r...</td>\n",
       "      <td>133259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hashtags</td>\n",
       "      <td>[[], ['#wagathachristie'], ['#abolisheton'], [...</td>\n",
       "      <td>6379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cashtags</td>\n",
       "      <td>[[], nan, https://twitter.com/stephenexley/sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>user_id</td>\n",
       "      <td>[13265, 397490403, 274620149, 19760125, 314483...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>user_id_str</td>\n",
       "      <td>[13265.0, 397490403.0, 274620149.0, 19760125.0...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>username</td>\n",
       "      <td>[RichardA, SianGriffiths6, seanjcoughlan, AnnM...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>name</td>\n",
       "      <td>[Richard Adams, Sian Griffiths, Sean Coughlan,...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>day</td>\n",
       "      <td>[4.0, 3.0, 1.0, 6.0, 2.0, 7.0, 5.0, nan]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hour</td>\n",
       "      <td>[13, 11, 10, 9, 8, 7, 6, 22, 19, 20, 14, 18, 1...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>link</td>\n",
       "      <td>[https://twitter.com/RichardA/status/497378170...</td>\n",
       "      <td>134142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>retweet</td>\n",
       "      <td>[False, nan]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nlikes</td>\n",
       "      <td>[0.0, 1.0, 2.0, 7.0, 12.0, 3.0, 6.0, 5.0, 13.0...</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nreplies</td>\n",
       "      <td>[2.0, 0.0, 1.0, 3.0, 4.0, 7.0, 5.0, 9.0, 33.0,...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nretweets</td>\n",
       "      <td>[0.0, 1.0, 5.0, 3.0, 8.0, 2.0, 7.0, 10.0, 21.0...</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>quote_url</td>\n",
       "      <td>[nan, https://twitter.com/REDROMINA/status/127...</td>\n",
       "      <td>11217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>search</td>\n",
       "      <td>[None, nan, [{'user_id': '50781408', 'username...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>near</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>geo</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>source</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>user_rt_id</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>user_rt</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>retweet_id</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>reply_to</td>\n",
       "      <td>[[{'user_id': '13265', 'username': 'RichardA'}...</td>\n",
       "      <td>34568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>retweet_date</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>translate</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>trans_src</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>trans_dest</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       column names                                      unique values  \\\n",
       "0        Unnamed: 0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1                id  [497378170757464065, 497376928446558208, 49736...   \n",
       "2   conversation_id  [497366362101415936, 497340002716897280, 49733...   \n",
       "3        created_at  [1407419171000, 1407418875000, 1407416761000, ...   \n",
       "4              date  [2014-08-07 13:46:11, 2014-08-07 13:41:15, 201...   \n",
       "5          timezone                                [UTC, stephenexley]   \n",
       "6             place  [nan, University of Oxford, Belfast City Hall,...   \n",
       "7             tweet  [@ajjolley it's not a DfE account, it's some r...   \n",
       "8          hashtags  [[], ['#wagathachristie'], ['#abolisheton'], [...   \n",
       "9          cashtags  [[], nan, https://twitter.com/stephenexley/sta...   \n",
       "10          user_id  [13265, 397490403, 274620149, 19760125, 314483...   \n",
       "11      user_id_str  [13265.0, 397490403.0, 274620149.0, 19760125.0...   \n",
       "12         username  [RichardA, SianGriffiths6, seanjcoughlan, AnnM...   \n",
       "13             name  [Richard Adams, Sian Griffiths, Sean Coughlan,...   \n",
       "14              day           [4.0, 3.0, 1.0, 6.0, 2.0, 7.0, 5.0, nan]   \n",
       "15             hour  [13, 11, 10, 9, 8, 7, 6, 22, 19, 20, 14, 18, 1...   \n",
       "16             link  [https://twitter.com/RichardA/status/497378170...   \n",
       "17          retweet                                       [False, nan]   \n",
       "18           nlikes  [0.0, 1.0, 2.0, 7.0, 12.0, 3.0, 6.0, 5.0, 13.0...   \n",
       "19         nreplies  [2.0, 0.0, 1.0, 3.0, 4.0, 7.0, 5.0, 9.0, 33.0,...   \n",
       "20        nretweets  [0.0, 1.0, 5.0, 3.0, 8.0, 2.0, 7.0, 10.0, 21.0...   \n",
       "21        quote_url  [nan, https://twitter.com/REDROMINA/status/127...   \n",
       "22           search  [None, nan, [{'user_id': '50781408', 'username...   \n",
       "23             near                                              [nan]   \n",
       "24              geo                                              [nan]   \n",
       "25           source                                              [nan]   \n",
       "26       user_rt_id                                              [nan]   \n",
       "27          user_rt                                              [nan]   \n",
       "28       retweet_id                                              [nan]   \n",
       "29         reply_to  [[{'user_id': '13265', 'username': 'RichardA'}...   \n",
       "30     retweet_date                                              [nan]   \n",
       "31        translate                                              [nan]   \n",
       "32        trans_src                                              [nan]   \n",
       "33       trans_dest                                              [nan]   \n",
       "\n",
       "    number of unique values  \n",
       "0                     19787  \n",
       "1                    134143  \n",
       "2                    108557  \n",
       "3                    133904  \n",
       "4                    133904  \n",
       "5                         2  \n",
       "6                        26  \n",
       "7                    133259  \n",
       "8                      6379  \n",
       "9                         4  \n",
       "10                       25  \n",
       "11                       23  \n",
       "12                       23  \n",
       "13                       23  \n",
       "14                        8  \n",
       "15                       50  \n",
       "16                   134142  \n",
       "17                        2  \n",
       "18                      289  \n",
       "19                       82  \n",
       "20                      190  \n",
       "21                    11217  \n",
       "22                        3  \n",
       "23                        1  \n",
       "24                        1  \n",
       "25                        1  \n",
       "26                        1  \n",
       "27                        1  \n",
       "28                        1  \n",
       "29                    34568  \n",
       "30                        1  \n",
       "31                        1  \n",
       "32                        1  \n",
       "33                        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a summary table fo unique information \n",
    "summary_table = pd.DataFrame(column_names)\n",
    "summary_table.columns=['column names']\n",
    "summary_table['unique values'] = unique_values\n",
    "summary_table['number of unique values'] = number_unique_values\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_callouts(text):\n",
    "    list_callouts = []\n",
    "    for i in range(0,text.shape[0]):\n",
    "        reply_temp = re.findall(r'\\@\\w+',df['tweet'][i])\n",
    "        list_callouts.append(reply_temp)\n",
    "    return list_callouts\n",
    "callouts = find_callouts(df['tweet'])\n",
    "df['callouts'] = callouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) #remove urls\n",
    "    text=re.sub(r'\\S+\\.com\\S+','',text) #remove urls\n",
    "    text=re.sub(r'\\@\\w+','',text) #remove mentions\n",
    "    text =re.sub(r'\\#\\w+','',text) #remove hashtags\n",
    "    return text\n",
    "df['clean_tweet'] = df['tweet'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from gensim) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: boto in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.35)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.35 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.35)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.35->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in c:\\users\\elizabeth\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.35->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "#This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(lowercase=True, \n",
    "                       min_df=20, \n",
    "                       stop_words='english', \n",
    "                       token_pattern= '(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b', \n",
    "                       ngram_range= (1,2)) # including bigrams\n",
    "\n",
    "text = vect.fit_transform(df['clean_tweet'])\n",
    "\n",
    "# Convert a matrix in scipy.sparse format into a streaming gensim corpus\n",
    "corpus = gensim.matutils.Sparse2Corpus(text, documents_columns=False)\n",
    "\n",
    "#Mapping from word IDs to words (to be used ub LdaModels id2word parameter)\n",
    "id_map =  dict((v, k) for k, v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.matutils.Sparse2Corpus"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"schools\" + 0.017*\"says\" + 0.014*\"new\" + 0.009*\"ofsted\" + 0.008*\"government\" + 0.008*\"social\" + 0.008*\"school\" + 0.008*\"teachers\" + 0.007*\"children\" + 0.007*\"report\"'),\n",
       " (1,\n",
       "  '0.017*\"going\" + 0.014*\"work\" + 0.012*\"time\" + 0.011*\"good\" + 0.008*\"great\" + 0.007*\"yes\" + 0.006*\"hard\" + 0.006*\"amazing\" + 0.005*\"just\" + 0.005*\"tomorrow\"'),\n",
       " (2,\n",
       "  '0.020*\"people\" + 0.015*\"day\" + 0.013*\"just\" + 0.012*\"picked\" + 0.011*\"thank\" + 0.009*\"young\" + 0.007*\"read\" + 0.007*\"way\" + 0.007*\"great\" + 0.006*\"got\"'),\n",
       " (3,\n",
       "  '0.021*\"health\" + 0.018*\"week\" + 0.010*\"just\" + 0.009*\"today\" + 0.007*\"school\" + 0.007*\"day\" + 0.007*\"special\" + 0.007*\"story\" + 0.006*\"mental\" + 0.006*\"read\"'),\n",
       " (4,\n",
       "  '0.028*\"school\" + 0.010*\"students\" + 0.010*\"free\" + 0.008*\"says\" + 0.008*\"think\" + 0.008*\"private\" + 0.007*\"don\" + 0.007*\"trust\" + 0.006*\"schools\" + 0.006*\"student\"'),\n",
       " (5,\n",
       "  '0.035*\"thanks\" + 0.016*\"years\" + 0.011*\"follow\" + 0.010*\"story\" + 0.009*\"email\" + 0.008*\"love\" + 0.007*\"send\" + 0.007*\"michael\" + 0.006*\"just\" + 0.006*\"early\"'),\n",
       " (6,\n",
       "  '0.024*\"year\" + 0.012*\"says\" + 0.012*\"schools\" + 0.011*\"students\" + 0.010*\"new\" + 0.009*\"teachers\" + 0.009*\"level\" + 0.009*\"gcse\" + 0.008*\"pupils\" + 0.006*\"results\"'),\n",
       " (7,\n",
       "  '0.019*\"funding\" + 0.016*\"news\" + 0.011*\"university\" + 0.011*\"new\" + 0.010*\"bbc\" + 0.010*\"looking\" + 0.008*\"education\" + 0.008*\"school\" + 0.007*\"forward\" + 0.007*\"schools\"'),\n",
       " (8,\n",
       "  '0.015*\"like\" + 0.014*\"good\" + 0.012*\"pay\" + 0.009*\"new\" + 0.008*\"long\" + 0.008*\"just\" + 0.008*\"looks\" + 0.008*\"dfe\" + 0.007*\"david\" + 0.006*\"time\"'),\n",
       " (9,\n",
       "  '0.017*\"know\" + 0.016*\"education\" + 0.015*\"don\" + 0.011*\"want\" + 0.010*\"good\" + 0.010*\"sure\" + 0.008*\"didn\" + 0.007*\"really\" + 0.007*\"big\" + 0.007*\"like\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 passes\n",
    "\n",
    "ldamodel = gensim.models.LdaMulticore(corpus, \n",
    "                                      passes= 2, # Number of passes through the corpus during training. (after optimising model increase to hundreds)\n",
    "                                      id2word=id_map, # Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "                                      random_state = 0, # becasue the gensim uses randomness in training (although also in inference)\n",
    "                                      num_topics=10, # number of topics to report\n",
    "                                      workers = 3) # number of workers should be one less than number of cores in your processor\n",
    "\n",
    "\n",
    "ldamodel.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.matutils.isbow(corpus)\n",
    "#Checks if vector passed is in bag of words representation or not. \n",
    "#Vec is considered to be in bag of words format if it is 2-tuple format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check that the Sparse2Corpus is transfering things into bag of words so have to revert back to this slow script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elizabeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elizabeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages for stopword removal \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is slow - perhaps because of iterating over the df?\n",
    "def normalized_text(text): #normalizing, stop word removal, & lementizing\n",
    "    normalized =[]\n",
    "    for i in range(0,text.shape[0]):\n",
    "        norm_temp = re.findall(r'[A-Za-z]+',text[i]) #taking all words (leaving punctuation out)\n",
    "        norm_temp = [w.lower() for w in norm_temp] # putting words in lower case\n",
    "        #norm_temp = [w for w in norm_temp if not w in stopwords.words(\"english\")] # removing stopwords\n",
    "        lemma = WordNetLemmatizer()\n",
    "        norm_temp = [lemma.lemmatize(w, pos = \"v\") for w in norm_temp] #lemmatizing verbs\n",
    "        norm_temp = [lemma.lemmatize(w, pos = \"n\") for w in norm_temp] #lemmatizing nouns\n",
    "        norm_temp = [' '.join(norm_temp)] \n",
    "        normalized.append(norm_temp)\n",
    "    return normalized\n",
    "\n",
    "normalized = normalized_text(df['clean_tweet'])\n",
    "df['normalized_tweet'] = normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising\n",
    "normalized_tweets = []\n",
    "for i in range(len(df['normalized_tweet'])):\n",
    "    normalized_tweets.append(nltk.word_tokenize(df['normalized_tweet'][i][0]))\n",
    "df['tokenized_tweets'] = normalized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it s not a dfe account it s some random unvalidated regional press office'],\n",
       " ['i agree the new rule say lowfat milk have to be available on sale the implication be that be then free to fsm and infant'],\n",
       " ['either a mistake or an elision milk be to be available and so free to those on fsm and infant in reception yr'],\n",
       " ['indeed henrietta barnet have a fantastic pedigree also just on fsm'],\n",
       " ['free school aren t selective so not the same dynamic'],\n",
       " ['btw this documentary on crossrail be geek fascinate'],\n",
       " ['interest ta george'],\n",
       " ['definitely a factor but not sufficient'],\n",
       " ['interest in how crossrail s bn be treat since it go all the way to read'],\n",
       " ['good question']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_corpus = for i in normalized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 account\n",
      "2 dfe\n",
      "3 it\n",
      "4 not\n",
      "5 office\n",
      "6 press\n",
      "7 random\n",
      "8 regional\n",
      "9 s\n",
      "10 some\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "#and this is from: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "dictionary = Dictionary(df['tokenized_tweets'])\n",
    "\n",
    "#and show the most freq words\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 2),\n",
       " (10, 1),\n",
       " (11, 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in df['tokenized_tweets']]\n",
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"a\") appears 1 time.\n",
      "Word 1 (\"account\") appears 1 time.\n",
      "Word 2 (\"dfe\") appears 1 time.\n",
      "Word 3 (\"it\") appears 2 time.\n",
      "Word 4 (\"not\") appears 1 time.\n",
      "Word 5 (\"office\") appears 1 time.\n",
      "Word 6 (\"press\") appears 1 time.\n",
      "Word 7 (\"random\") appears 1 time.\n",
      "Word 8 (\"regional\") appears 1 time.\n",
      "Word 9 (\"s\") appears 2 time.\n",
      "Word 10 (\"some\") appears 1 time.\n",
      "Word 11 (\"unvalidated\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag Of Words for our sample preprocessed document.\n",
    "bow_doc_0 = bow_corpus[0]\n",
    "for i in range(len(bow_doc_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "                                               dictionary[bow_doc_0[i][0]], \n",
    "                                                     bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"get\" + 0.009*\"email\" + 0.009*\"love\" + 0.009*\"please\" + 0.008*\"thank\" + 0.008*\"gove\" + 0.008*\"hi\" + 0.007*\"sir\" + 0.006*\"uk\" + 0.006*\"work\"\n",
      "Topic: 1 \n",
      "Words: 0.010*\"via\" + 0.010*\"rt\" + 0.008*\"one\" + 0.008*\"award\" + 0.008*\"look\" + 0.008*\"action\" + 0.007*\"take\" + 0.006*\"prize\" + 0.006*\"win\" + 0.006*\"say\"\n",
      "Topic: 2 \n",
      "Words: 0.027*\"good\" + 0.016*\"news\" + 0.015*\"well\" + 0.013*\"know\" + 0.010*\"get\" + 0.009*\"do\" + 0.009*\"make\" + 0.008*\"say\" + 0.008*\"bbc\" + 0.008*\"go\"\n",
      "Topic: 3 \n",
      "Words: 0.020*\"health\" + 0.014*\"look\" + 0.012*\"one\" + 0.008*\"need\" + 0.008*\"child\" + 0.007*\"would\" + 0.007*\"school\" + 0.007*\"say\" + 0.007*\"help\" + 0.007*\"via\"\n",
      "Topic: 4 \n",
      "Words: 0.013*\"thank\" + 0.012*\"could\" + 0.011*\"follow\" + 0.009*\"dm\" + 0.009*\"please\" + 0.009*\"nice\" + 0.008*\"think\" + 0.008*\"one\" + 0.008*\"agree\" + 0.008*\"would\"\n",
      "Topic: 5 \n",
      "Words: 0.023*\"thank\" + 0.015*\"see\" + 0.011*\"brexit\" + 0.010*\"apprenticeship\" + 0.009*\"think\" + 0.008*\"nh\" + 0.007*\"great\" + 0.007*\"get\" + 0.007*\"may\" + 0.006*\"need\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"student\" + 0.018*\"university\" + 0.018*\"year\" + 0.018*\"pay\" + 0.016*\"via\" + 0.013*\"say\" + 0.010*\"time\" + 0.010*\"uk\" + 0.007*\"college\" + 0.006*\"rise\"\n",
      "Topic: 7 \n",
      "Words: 0.015*\"great\" + 0.012*\"today\" + 0.011*\"read\" + 0.011*\"sign\" + 0.010*\"week\" + 0.009*\"think\" + 0.009*\"story\" + 0.008*\"top\" + 0.008*\"take\" + 0.008*\"last\"\n",
      "Topic: 8 \n",
      "Words: 0.017*\"new\" + 0.012*\"education\" + 0.012*\"fund\" + 0.010*\"via\" + 0.008*\"school\" + 0.007*\"pick\" + 0.007*\"follow\" + 0.007*\"say\" + 0.007*\"sure\" + 0.006*\"fe\"\n",
      "Topic: 9 \n",
      "Words: 0.047*\"school\" + 0.025*\"via\" + 0.021*\"say\" + 0.013*\"teacher\" + 0.011*\"new\" + 0.011*\"pupil\" + 0.010*\"ofsted\" + 0.008*\"dfe\" + 0.007*\"academy\" + 0.007*\"fund\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=10, \n",
    "                                       id2word=dictionary, \n",
    "                                       passes=2, \n",
    "                                       workers=3,\n",
    "                                       random_state = 0)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if topics are still different: \n",
    "\n",
    "SOME_FIXED_SEED = 42\n",
    "\n",
    "# before training/inference:\n",
    "np.random.seed(SOME_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf model object using models.TfidfModel on 'corpus’ and save it to ‘tfidf’, \n",
    "# TfidfModel is short for term frequency-inverse document frequency: a numeric stat reflecting how important a word is\n",
    "# increases proportionally to the number of times a word appears in a doc.\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# then apply transformation to the entire corpus and call it ‘corpus_tfidf’\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=10, \n",
    "                                             id2word=id_map, \n",
    "                                             passes=2, \n",
    "                                             workers=3, \n",
    "                                             random_state= 0)\n",
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\n Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the LDA model \n",
    "\n",
    "# check where our test document would be classified\n",
    "\n",
    "print(corpus[4310])\n",
    "\n",
    "#for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "#    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the TF-IDF model (K-means clustering)\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
